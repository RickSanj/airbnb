{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine\n",
    "from pyspark.sql.functions import isnan, when, count, col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/23 13:26:15 WARN Utils: Your hostname, MacBook-Pro-Anna-3.local resolves to a loopback address: 127.0.0.1; using 192.168.0.100 instead (on interface en0)\n",
      "24/11/23 13:26:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/23 13:26:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.100:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Proj</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x12d98a320>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Proj\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## contacts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/23 13:26:29 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+----------------+-----------------+--------+--------------+------------------------------------+---------------------+----------------------+\n",
      "|       id_guest_anon|        id_host_anon|     id_listing_anon|ts_interaction_first|  ts_reply_at_first|ts_accepted_at_first|      ts_booking_at|ds_checkin_first|ds_checkout_first|m_guests|m_interactions|m_first_message_length_in_characters|contact_channel_first|guest_user_stage_first|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+----------------+-----------------+--------+--------------+------------------------------------+---------------------+----------------------+\n",
      "|da8656a1-51af-4f3...|5426897d-960d-401...|a408a8b2-0d44-451...| 2016-04-21 02:55:53|2016-04-21 03:15:00| 2016-04-21 03:15:00|2016-04-21 03:15:00|      2016-08-02|       2016-08-06|     4.0|            30|                                31.0|              book_it|           past_booker|\n",
      "|8590d6f1-8bc9-4e8...|f30417c5-6df4-45a...|e387c705-0aeb-464...| 2016-02-16 22:14:01|2016-02-16 23:37:36|                NULL|               NULL|      2016-08-11|       2016-08-22|     5.0|             2|                               312.0|           contact_me|           past_booker|\n",
      "|ebcd83ba-bda1-47e...|13cbf50a-3272-45d...|d1eb1960-938f-430...| 2016-01-27 23:33:38|2016-01-28 02:12:47|                NULL|               NULL|      2016-03-14|       2016-03-23|     1.0|             3|                               164.0|           contact_me|                   new|\n",
      "|b0af8848-fe2a-4ef...|01614601-d5a4-477...|855f6779-346c-45f...| 2016-05-05 14:42:52|2016-05-05 15:17:40| 2016-05-05 15:17:40|               NULL|      2016-05-27|       2016-05-29|     2.0|             3|                               203.0|           contact_me|                   new|\n",
      "|5ddbbcc3-ac1a-4d8...|f2fed6f3-4c5c-453...|f2928a59-c5e7-42b...| 2016-06-23 03:09:25|2016-06-23 03:09:26| 2016-06-23 03:09:33|2016-06-23 03:09:33|      2016-08-19|       2016-08-21|     3.0|            25|                                 0.0|         instant_book|           past_booker|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+----------------+-----------------+--------+--------------+------------------------------------+---------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_con = spark.read.option('header', 'true').csv(\"data/contacts.csv\", inferSchema=True)\n",
    "df_con.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_guest_anon: string (nullable = true)\n",
      " |-- id_host_anon: string (nullable = true)\n",
      " |-- id_listing_anon: string (nullable = true)\n",
      " |-- ts_interaction_first: timestamp (nullable = true)\n",
      " |-- ts_reply_at_first: timestamp (nullable = true)\n",
      " |-- ts_accepted_at_first: timestamp (nullable = true)\n",
      " |-- ts_booking_at: timestamp (nullable = true)\n",
      " |-- ds_checkin_first: date (nullable = true)\n",
      " |-- ds_checkout_first: date (nullable = true)\n",
      " |-- m_guests: double (nullable = true)\n",
      " |-- m_interactions: integer (nullable = true)\n",
      " |-- m_first_message_length_in_characters: double (nullable = true)\n",
      " |-- contact_channel_first: string (nullable = true)\n",
      " |-- guest_user_stage_first: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_con.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "count_dup_rows = df_con.count() - df_con.distinct().count()\n",
    "print(f\"Number of duplicate rows: {count_dup_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Null values in each column: \n",
      "id_guest_anon :  0\n",
      "id_host_anon :  0\n",
      "id_listing_anon :  0\n",
      "ts_interaction_first :  0\n",
      "ts_reply_at_first :  2032\n",
      "ts_accepted_at_first :  11472\n",
      "ts_booking_at :  16300\n",
      "ds_checkin_first :  0\n",
      "ds_checkout_first :  0\n",
      "m_guests :  1\n",
      "m_interactions :  0\n",
      "m_first_message_length_in_characters :  0\n",
      "contact_channel_first :  0\n",
      "guest_user_stage_first :  0\n"
     ]
    }
   ],
   "source": [
    "Dict_Null = {col:df_con.filter(df_con[col].isNull()).count() for col in df_con.columns}\n",
    "print(f\"Number of Null values in each column: \")\n",
    "for key, value in Dict_Null.items():\n",
    "    print(key, \": \", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## listings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+-------------+\n",
      "|     id_listing_anon|      room_type|listing_neighborhood|total_reviews|\n",
      "+--------------------+---------------+--------------------+-------------+\n",
      "|71582793-e5f8-46d...|   Private room|           -unknown-|          0.0|\n",
      "|a1a3f728-e21f-443...|Entire home/apt|          Copacabana|          0.0|\n",
      "|353a68be-ecf9-4b7...|Entire home/apt|     Barra da Tijuca|          3.0|\n",
      "|b9ae1908-0486-40a...|Entire home/apt|                Lapa|          4.0|\n",
      "|fa0290ef-7881-448...|Entire home/apt|           -unknown-|          0.0|\n",
      "+--------------------+---------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lis = spark.read.option('header', 'true').csv(\"data/listings.csv\", inferSchema=True)\n",
    "df_lis.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_listing_anon: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- listing_neighborhood: string (nullable = true)\n",
      " |-- total_reviews: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lis.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "count_dup_rows = df_lis.count() - df_lis.distinct().count()\n",
    "print(f\"Number of duplicate rows: {count_dup_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Null values in each column: \n",
      "id_listing_anon :  0\n",
      "room_type :  0\n",
      "listing_neighborhood :  0\n",
      "total_reviews :  0\n"
     ]
    }
   ],
   "source": [
    "Dict_Null = {col:df_lis.filter(df_lis[col].isNull()).count() for col in df_lis.columns}\n",
    "print(f\"Number of Null values in each column: \")\n",
    "for key, value in Dict_Null.items():\n",
    "    print(key, \": \", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## users.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------------------+\n",
      "|        id_user_anon|country|words_in_user_profile|\n",
      "+--------------------+-------+---------------------+\n",
      "|1d16a001-31a2-494...|     FR|                    0|\n",
      "|42607e0a-86c0-472...|     AR|                    0|\n",
      "|25f85eb5-a700-44e...|     BR|                    0|\n",
      "|55abeba0-18ef-4c5...|     BR|                    1|\n",
      "|5d62d35a-7d6d-45d...|     BR|                   98|\n",
      "+--------------------+-------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user = spark.read.option('header', 'true').csv(\"data/users.csv\", inferSchema=True)\n",
    "df_user.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_user_anon: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- words_in_user_profile: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 68\n"
     ]
    }
   ],
   "source": [
    "count_dup_rows = df_user.count() - df_user.distinct().count()\n",
    "print(f\"Number of duplicate rows: {count_dup_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = df_user.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "count_dup_rows = df_user.count() - df_user.distinct().count()\n",
    "print(f\"Number of duplicate rows: {count_dup_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Null values in each column: \n",
      "id_user_anon :  0\n",
      "country :  0\n",
      "words_in_user_profile :  0\n"
     ]
    }
   ],
   "source": [
    "Dict_Null = {col:df_user.filter(df_user[col].isNull()).count() for col in df_user.columns}\n",
    "print(f\"Number of Null values in each column: \")\n",
    "for key, value in Dict_Null.items():\n",
    "    print(key, \": \", value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
